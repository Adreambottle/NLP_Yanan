{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " 'Arthur',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'feel',\n",
       " 'very',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenlization: nltk.word_tokenlization\n",
    "sentence = \"\"\"\n",
    "At eight o'clock on Thursday morning\n",
    "Arthur didn't feel very good.\n",
    "\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('At', 'IN'),\n",
       " ('eight', 'CD'),\n",
       " (\"o'clock\", 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Thursday', 'NNP'),\n",
       " ('morning', 'NN'),\n",
       " ('Arthur', 'NNP'),\n",
       " ('did', 'VBD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('feel', 'VB'),\n",
       " ('very', 'RB'),\n",
       " ('good', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add parser tokens for the taggs\n",
    "taggs = nltk.pos_tag(tokens)\n",
    "taggs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.tokenize, nltk.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "\n",
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n",
    "... two of them.\\n\\nThanks.'''\n",
    "\n",
    "# tokenized to the word\n",
    "word_tokenize(s)\n",
    "\n",
    "# tokenized to the punct word\n",
    "wordpunct_tokenize(s)\n",
    "\n",
    "# tokenized to the sentence\n",
    "sent_tokenize(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4),\n",
       " (5, 12),\n",
       " (13, 17),\n",
       " (18, 23),\n",
       " (24, 26),\n",
       " (27, 30),\n",
       " (31, 36),\n",
       " (38, 44),\n",
       " (45, 48),\n",
       " (49, 51),\n",
       " (52, 55),\n",
       " (56, 59),\n",
       " (60, 62),\n",
       " (63, 68),\n",
       " (70, 77)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "# WhitespaceTokenizer 返回每个单词的起始和终止的位置\n",
    "list(WhitespaceTokenizer().span_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caress',\n",
       " 'fli',\n",
       " 'die',\n",
       " 'mule',\n",
       " 'deni',\n",
       " 'die',\n",
       " 'agre',\n",
       " 'own',\n",
       " 'humbl',\n",
       " 'size',\n",
       " 'meet',\n",
       " 'state',\n",
       " 'siez',\n",
       " 'item',\n",
       " 'sensat',\n",
       " 'tradit',\n",
       " 'refer',\n",
       " 'colon',\n",
       " 'plot']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.stem 提取单词的主干，删去词缀\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "singles\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60951fe2ccac1c258907b209f658f6ef884c93d9ccb3f36667a6035df696e51a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
